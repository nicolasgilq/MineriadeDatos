{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6\n",
    "\n",
    "## SVM & Regularization\n",
    "\n",
    "\n",
    "For this homework we consider a set of observations on a number of red and white wine varieties involving their chemical properties and ranking by tasters. Wine industry shows a recent growth spurt as social drinking is on the rise. The price of wine depends on a rather abstract concept of wine appreciation by wine tasters, opinion among whom may have a high degree of variability. Pricing of wine depends on such a volatile factor to some extent. Another key factor in wine certification and quality assessment is physicochemical tests which are laboratory-based and takes into account factors like acidity, pH level, presence of sugar and other chemical properties. For the wine market, it would be of interest if human quality of tasting can be related to the chemical properties of wine so that certification and quality assessment and assurance process is more controlled.\n",
    "\n",
    "Two datasets are available of which one dataset is on red wine and have 1599 different varieties and the other is on white wine and have 4898 varieties. All wines are produced in a particular area of Portugal. Data are collected on 12 different properties of the wines one of which is Quality, based on sensory data, and the rest are on chemical properties of the wines including density, acidity, alcohol content etc. All chemical properties of wines are continuous variables. Quality is an ordinal variable with possible ranking from 1 (worst) to 10 (best). Each variety of wine is tasted by three independent tasters and the final rank assigned is the median rank given by the tasters.\n",
    "\n",
    "A predictive model developed on this data is expected to provide guidance to vineyards regarding quality and price expected on their produce without heavy reliance on volatility of wine tasters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_r = pd.read_csv('https://github.com/albahnsen/PracticalMachineLearningClass/raw/master/datasets/Wine_data_red.csv')\n",
    "data_w = pd.read_csv('https://github.com/albahnsen/PracticalMachineLearningClass/raw/master/datasets/Wine_data_white.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5991</th>\n",
       "      <td>9.2</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.36</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.079</td>\n",
       "      <td>11.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.99615</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.86</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4201</th>\n",
       "      <td>6.7</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.27</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.033</td>\n",
       "      <td>25.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.99086</td>\n",
       "      <td>3.05</td>\n",
       "      <td>0.34</td>\n",
       "      <td>11.7</td>\n",
       "      <td>6</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4928</th>\n",
       "      <td>6.7</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.07</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.089</td>\n",
       "      <td>17.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.99580</td>\n",
       "      <td>3.35</td>\n",
       "      <td>0.54</td>\n",
       "      <td>10.1</td>\n",
       "      <td>5</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1863</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.30</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.049</td>\n",
       "      <td>57.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>0.99580</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.39</td>\n",
       "      <td>9.5</td>\n",
       "      <td>5</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3457</th>\n",
       "      <td>6.1</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.037</td>\n",
       "      <td>55.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>0.99120</td>\n",
       "      <td>3.41</td>\n",
       "      <td>0.41</td>\n",
       "      <td>11.1</td>\n",
       "      <td>7</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "5991            9.2             0.310         0.36             2.2      0.079   \n",
       "4201            6.7             0.410         0.27             2.6      0.033   \n",
       "4928            6.7             0.675         0.07             2.4      0.089   \n",
       "1863            6.6             0.280         0.30             7.8      0.049   \n",
       "3457            6.1             0.105         0.31             1.3      0.037   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "5991                 11.0                  31.0  0.99615  3.33       0.86   \n",
       "4201                 25.0                  85.0  0.99086  3.05       0.34   \n",
       "4928                 17.0                  82.0  0.99580  3.35       0.54   \n",
       "1863                 57.0                 202.0  0.99580  3.24       0.39   \n",
       "3457                 55.0                 145.0  0.99120  3.41       0.41   \n",
       "\n",
       "      alcohol  quality   type  \n",
       "5991     12.0        7    red  \n",
       "4201     11.7        6  white  \n",
       "4928     10.1        5    red  \n",
       "1863      9.5        5  white  \n",
       "3457     11.1        7  white  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data_w.assign(type = 'white')\n",
    "\n",
    "data = data.append(data_r.assign(type = 'red'), ignore_index=True)\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6.1\n",
    "\n",
    "Show the frecuency table of the quality by type of wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red wine \n",
      " 3     10\n",
      "4     53\n",
      "5    681\n",
      "6    638\n",
      "7    199\n",
      "8     18\n",
      "Name: quality, dtype: int64\n",
      "white wine \n",
      " 3      20\n",
      "4     163\n",
      "5    1457\n",
      "6    2198\n",
      "7     880\n",
      "8     175\n",
      "9       5\n",
      "Name: quality, dtype: int64\n",
      "Red and white\n",
      " 3      30\n",
      "4     216\n",
      "5    2138\n",
      "6    2836\n",
      "7    1079\n",
      "8     193\n",
      "9       5\n",
      "Name: quality, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Red Wine #\n",
    "\n",
    "freq_tab_r = pd.value_counts(data_r[\"quality\"]).sort_index()\n",
    "print(\"red wine \\n\",freq_tab_r)\n",
    "\n",
    "# White Wine #\n",
    "freq_tab_w = pd.value_counts(data_w[\"quality\"]).sort_index()\n",
    "print(\"white wine \\n\",freq_tab_w)\n",
    "\n",
    "# Red and white\n",
    "freq_tab = pd.value_counts(data[\"quality\"]).sort_index()\n",
    "print(\"Red and white\\n\",freq_tab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay 7 categorias en total que clasifica el vino de acuerdo a la calidad. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6.2\n",
    "\n",
    "* Standarized the features (not the quality)\n",
    "* Create a binary target for each type of wine\n",
    "* Create two Linear SVM's for the white and red wines, repectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC # \"Support Vector Classifier\"\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score, f1_score, accuracy_score, confusion_matrix, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "1            6.3              0.30         0.34             1.6      0.049   \n",
       "2            8.1              0.28         0.40             6.9      0.050   \n",
       "3            7.2              0.23         0.32             8.5      0.058   \n",
       "4            7.2              0.23         0.32             8.5      0.058   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "\n",
       "   alcohol  quality  type  \n",
       "0      8.8        6     0  \n",
       "1      9.5        6     0  \n",
       "2     10.1        6     0  \n",
       "3      9.9        6     0  \n",
       "4      9.9        6     0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Punto b \n",
    "data_r = data_r.assign(type=1)\n",
    "data_w = data_w.assign(type=0)\n",
    "data_r.head()\n",
    "data_w.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a\n",
    "# Red Wine\n",
    "x1_1 = data_r['fixed acidity'].values.reshape(-1, 1)\n",
    "x1_2 = data_r['volatile acidity'].values.reshape(-1, 1)\n",
    "x1_3 = data_r['citric acid'].values.reshape(-1, 1)\n",
    "x1_4 = data_r['residual sugar'].values.reshape(-1, 1)\n",
    "x1_5 = data_r['chlorides'].values.reshape(-1, 1)\n",
    "x1_6 = data_r['free sulfur dioxide'].values.reshape(-1, 1)\n",
    "x1_7 = data_r['total sulfur dioxide'].values.reshape(-1, 1)\n",
    "x1_8 = data_r['density'].values.reshape(-1, 1)\n",
    "x1_9 = data_r['pH'].values.reshape(-1, 1)\n",
    "x1_10 = data_r['sulphates'].values.reshape(-1, 1)\n",
    "x1_11 = data_r['alcohol'].values.reshape(-1, 1)\n",
    "x1_12 = data_r['type'].values.reshape(-1, 1)\n",
    "X1 = np.c_[x1_1, x1_2, x1_3, x1_4, x1_5, x1_6, x1_7, x1_8, x1_9, x1_10, x1_11, x1_12]\n",
    "\n",
    "ss1 = StandardScaler(with_mean=True, with_std=True)\n",
    "ss1.fit(X1.astype(np.float))\n",
    "X1_n = ss1.transform(X1.astype(np.float))\n",
    "\n",
    "Y1 = data_r[\"quality\"]\n",
    "\n",
    "# White Wine\n",
    "x2_1 = data_w['fixed acidity'].values.reshape(-1, 1)\n",
    "x2_2 = data_w['volatile acidity'].values.reshape(-1, 1)\n",
    "x2_3 = data_w['citric acid'].values.reshape(-1, 1)\n",
    "x2_4 = data_w['residual sugar'].values.reshape(-1, 1)\n",
    "x2_5 = data_w['chlorides'].values.reshape(-1, 1)\n",
    "x2_6 = data_w['free sulfur dioxide'].values.reshape(-1, 1)\n",
    "x2_7 = data_w['total sulfur dioxide'].values.reshape(-1, 1)\n",
    "x2_8 = data_w['density'].values.reshape(-1, 1)\n",
    "x2_9 = data_w['pH'].values.reshape(-1, 1)\n",
    "x2_10 = data_w['sulphates'].values.reshape(-1, 1)\n",
    "x2_11 = data_w['alcohol'].values.reshape(-1, 1)\n",
    "x2_12 = data_w['type'].values.reshape(-1, 1)\n",
    "X2 = np.c_[x2_1, x2_2, x2_3, x2_4, x2_5, x2_6, x2_7, x2_8, x2_9, x2_10, x2_11, x2_12]\n",
    "\n",
    "ss2 = StandardScaler(with_mean=True, with_std=True)\n",
    "ss2.fit(X2.astype(np.float))\n",
    "X2_n = ss2.transform(X2.astype(np.float))\n",
    "\n",
    "Y2 = data_w[\"quality\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Red_linear\n",
      "\n",
      "Accuracy: 0.5866166353971232\n",
      "Confusion Matrix: \n",
      " [[  0   0   9   1   0   0]\n",
      " [  0   0  39  14   0   0]\n",
      " [  0   0 529 152   0   0]\n",
      " [  0   0 229 409   0   0]\n",
      " [  0   0  13 186   0   0]\n",
      " [  0   0   0  18   0   0]]\n",
      "\n",
      "White_linear\n",
      "\n",
      "Accuracy: 0.5216414863209473\n",
      "Confusion Matrix: \n",
      " [[   0    0    8   12    0    0    0]\n",
      " [   0    0  102   61    0    0    0]\n",
      " [   0    0  780  677    0    0    0]\n",
      " [   0    0  423 1775    0    0    0]\n",
      " [   0    0   43  837    0    0    0]\n",
      " [   0    0    1  174    0    0    0]\n",
      " [   0    0    0    5    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "# c\n",
    "# Red Wine\n",
    "#   a. linear\n",
    "clf1 = SVC(kernel='linear')\n",
    "clf1.fit(X1_n, Y1) #Ajustar el modelo\n",
    "\n",
    "Y1_predictions = clf1.predict(X1_n)\n",
    "print('\\nRed_linear\\n')\n",
    "print(\"Accuracy:\", accuracy_score(Y1, Y1_predictions))  \n",
    "print('Confusion Matrix: \\n', confusion_matrix(Y1, Y1_predictions))\n",
    "\n",
    "# White Wine\n",
    "#   a. linear\n",
    "clf2 = SVC(kernel='linear')\n",
    "clf2.fit(X2_n, Y2)\n",
    "\n",
    "Y2_predictions = clf2.predict(X2_n)\n",
    "\n",
    "print('\\nWhite_linear\\n')\n",
    "print(\"Accuracy:\", accuracy_score(Y2, Y2_predictions))  \n",
    "print('Confusion Matrix: \\n', confusion_matrix(Y2, Y2_predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RTA/**La maquina de soporte vectorial se ajusto bien a esos datos. El 58% de los datos del vino rojo se ajustaron correctamente, mientras que con el vino blanco se ajustaron solo el 52%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6.3\n",
    "\n",
    "Test the two SVM's using the different kernels (‘poly’, ‘rbf’, ‘sigmoid’)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Red_poly\n",
      "\n",
      "Accuracy: 0.6760475297060663\n",
      "Confusion Matrix: \n",
      " [[  3   0   5   2   0   0]\n",
      " [  0   7  36   9   1   0]\n",
      " [  0   0 575 105   1   0]\n",
      " [  0   0 211 419   8   0]\n",
      " [  0   0  15 109  75   0]\n",
      " [  0   0   0  12   4   2]]\n",
      "\n",
      "white_poly\n",
      "\n",
      "Accuracy: 0.5681910984075133\n",
      "Confusion Matrix: \n",
      " [[  11    0    3    6    0    0    0]\n",
      " [   0   36   58   69    0    0    0]\n",
      " [   0    5  611  838    3    0    0]\n",
      " [   0    1  184 1971   42    0    0]\n",
      " [   0    0   15  717  146    2    0]\n",
      " [   0    0    0  131   36    8    0]\n",
      " [   0    0    0    3    2    0    0]]\n"
     ]
    }
   ],
   "source": [
    "clf3a = SVC(kernel='poly') #polinomico\n",
    "clf3a.fit(X1_n, Y1) #Ajustar el modelo\n",
    "Y3a_predictions = clf3a.predict(X1_n)\n",
    "print('\\nRed_poly\\n')\n",
    "print(\"Accuracy:\", accuracy_score(Y1, Y3a_predictions))  \n",
    "print('Confusion Matrix: \\n', confusion_matrix(Y1, Y3a_predictions))\n",
    "\n",
    "clf3b = SVC(kernel='poly')\n",
    "clf3b.fit(X2_n, Y2)\n",
    "Y3b_predictions = clf3b.predict(X2_n)\n",
    "print('\\nwhite_poly\\n')\n",
    "print(\"Accuracy:\", accuracy_score(Y2, Y3b_predictions))  \n",
    "print('Confusion Matrix: \\n', confusion_matrix(Y2, Y3b_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RTA/**Cuando corremos el modelo mediante una regresion polinomica, encontramos para el vino rojo un ajuste del 67.60% y del vino blanco 56,81%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Red_poly\n",
      "\n",
      "Accuracy: 0.66729205753596\n",
      "Confusion Matrix: \n",
      " [[  0   0   7   3   0   0]\n",
      " [  0   1  38  13   1   0]\n",
      " [  0   0 545 134   2   0]\n",
      " [  0   0 171 450  17   0]\n",
      " [  0   0  12 116  71   0]\n",
      " [  0   0   0  12   6   0]]\n",
      "\n",
      "white_poly\n",
      "\n",
      "Accuracy: 0.6108615761535321\n",
      "Confusion Matrix: \n",
      " [[   1    0    8   11    0    0    0]\n",
      " [   0   17   98   46    2    0    0]\n",
      " [   0    1  927  525    4    0    0]\n",
      " [   0    0  344 1778   76    0    0]\n",
      " [   0    0   28  583  269    0    0]\n",
      " [   0    0    1  127   47    0    0]\n",
      " [   0    0    0    3    2    0    0]]\n"
     ]
    }
   ],
   "source": [
    "clf3c = SVC(kernel='rbf')#exponencial\n",
    "clf3c.fit(X1_n, Y1) #Ajustar el modelo\n",
    "Y3c_predictions = clf3c.predict(X1_n)\n",
    "print('\\nRed_poly\\n')\n",
    "print(\"Accuracy:\", accuracy_score(Y1, Y3c_predictions))  \n",
    "print('Confusion Matrix: \\n', confusion_matrix(Y1, Y3c_predictions))\n",
    "\n",
    "clf3d = SVC(kernel='rbf')\n",
    "clf3d.fit(X2_n, Y2)\n",
    "Y3d_predictions = clf3d.predict(X2_n)\n",
    "print('\\nwhite_poly\\n')\n",
    "print(\"Accuracy:\", accuracy_score(Y2, Y3d_predictions))  \n",
    "print('Confusion Matrix: \\n', confusion_matrix(Y2, Y3d_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RTA/**Cuando corremos el modelo mediante una regresion exponencial, encontramos para el vino rojo un ajuste del 66.72% y del vino blanco 61,08%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Red_poly\n",
      "\n",
      "Accuracy: 0.49468417761100686\n",
      "Confusion Matrix: \n",
      " [[  0   0   3   7   0   0]\n",
      " [  0   0  27  22   3   1]\n",
      " [  0   5 408 230  38   0]\n",
      " [  0   2 215 341  80   0]\n",
      " [  0   0  34 123  42   0]\n",
      " [  0   0   2   8   8   0]]\n",
      "\n",
      "white_poly\n",
      "\n",
      "Accuracy: 0.42343813801551655\n",
      "Confusion Matrix: \n",
      " [[   0    2    9    6    3    0    0]\n",
      " [   1   15   73   66    6    2    0]\n",
      " [   0  104  635  647   68    3    0]\n",
      " [   1   71  599 1163  352   12    0]\n",
      " [   0   11  100  497  257   15    0]\n",
      " [   0    3    9  105   54    4    0]\n",
      " [   0    0    1    0    4    0    0]]\n"
     ]
    }
   ],
   "source": [
    "clf3e = SVC(kernel='sigmoid') #0 \n",
    "clf3e.fit(X1_n, Y1) #Ajustar el modelo\n",
    "Y3e_predictions = clf3e.predict(X1_n)\n",
    "print('\\nRed_poly\\n')\n",
    "print(\"Accuracy:\", accuracy_score(Y1, Y3e_predictions))  \n",
    "print('Confusion Matrix: \\n', confusion_matrix(Y1, Y3e_predictions))\n",
    "\n",
    "clf3f = SVC(kernel='sigmoid')\n",
    "clf3f.fit(X2_n, Y2)\n",
    "Y3f_predictions = clf3f.predict(X2_n)\n",
    "print('\\nwhite_poly\\n')\n",
    "print(\"Accuracy:\", accuracy_score(Y2, Y3f_predictions))  \n",
    "print('Confusion Matrix: \\n', confusion_matrix(Y2, Y3f_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RTA/**Cuando corremos el modelo mediante una regresion sigmoid, encontramos para el vino rojo un ajuste del 49.46% y del vino blanco 42.34%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6.4\n",
    "Using the best SVM find the parameters that gives the best performance\n",
    "\n",
    "'C': [0.1, 1, 10, 100, 1000], 'gamma': [0.01, 0.001, 0.0001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = [0.1, 1, 10, 100, 1000] #parametros de ajustes\n",
    "gamma = [0.01, 0.001, 0.0001]\n",
    "\n",
    "r_acc_scores = []\n",
    "w_acc_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.01\n",
      "0.425891181988743\n",
      "0.1\n",
      "0.001\n",
      "0.425891181988743\n",
      "0.1\n",
      "0.0001\n",
      "0.425891181988743\n",
      "1\n",
      "0.01\n",
      "0.42964352720450283\n",
      "1\n",
      "0.001\n",
      "0.425891181988743\n",
      "1\n",
      "0.0001\n",
      "0.425891181988743\n",
      "10\n",
      "0.01\n",
      "0.4546591619762351\n",
      "10\n",
      "0.001\n",
      "0.425891181988743\n",
      "10\n",
      "0.0001\n",
      "0.425891181988743\n",
      "100\n",
      "0.01\n",
      "0.5872420262664165\n",
      "100\n",
      "0.001\n",
      "0.425891181988743\n",
      "100\n",
      "0.0001\n",
      "0.425891181988743\n",
      "1000\n",
      "0.01\n",
      "0.6891807379612258\n",
      "1000\n",
      "0.001\n",
      "0.42964352720450283\n",
      "1000\n",
      "0.0001\n",
      "0.425891181988743\n"
     ]
    }
   ],
   "source": [
    "for i in C:\n",
    "    for j in gamma:\n",
    "        # Red Wine\n",
    "        clf_4_r = SVC(C=i, gamma=j, kernel='poly')\n",
    "        clf_4_r.fit(X1_n, Y1)\n",
    "        Y_4_r_predictions = clf_4_r.predict(X1_n)\n",
    "        r_acc_val = accuracy_score(Y1, Y_4_r_predictions)\n",
    "        r_acc_scores.append(r_acc_val) #en las lineas pasadas se genera el valor y en este se almacena\n",
    "        print(i)\n",
    "        print(j)\n",
    "        print(r_acc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RTA/**\n",
    "Para el vino blanco utilizamos la regresion pilinomica, revisando los resultados de cada combinacion posible, modificando el C y el gamma el mejor ajuste se da cuando c=100 y gamma=0.01, arrojando un ajuste del 68,91%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.01\n",
      "0.504695794201715\n",
      "0.1\n",
      "0.001\n",
      "0.44875459371171905\n",
      "0.1\n",
      "0.0001\n",
      "0.44875459371171905\n",
      "1\n",
      "0.01\n",
      "0.5371580236831359\n",
      "1\n",
      "0.001\n",
      "0.5048999591670069\n",
      "1\n",
      "0.0001\n",
      "0.44875459371171905\n",
      "10\n",
      "0.01\n",
      "0.5673744385463454\n",
      "10\n",
      "0.001\n",
      "0.5244997958350347\n",
      "10\n",
      "0.0001\n",
      "0.504695794201715\n",
      "100\n",
      "0.01\n",
      "0.5998366680277665\n",
      "100\n",
      "0.001\n",
      "0.5398121682319315\n",
      "100\n",
      "0.0001\n",
      "0.5210289914250714\n",
      "1000\n",
      "0.01\n",
      "0.6412821559820335\n",
      "1000\n",
      "0.001\n",
      "0.5653327888934259\n",
      "1000\n",
      "0.0001\n",
      "0.5267456104532462\n"
     ]
    }
   ],
   "source": [
    " for i in C:\n",
    "    for j in gamma:\n",
    "    # White Wine\n",
    "        clf_4_w = SVC(C=i, gamma=j, kernel='rbf')\n",
    "        clf_4_w.fit(X2_n, Y2)\n",
    "        Y_4_w_predictions = clf_4_w.predict(X2_n)\n",
    "        w_acc_val = accuracy_score(Y2, Y_4_w_predictions)\n",
    "        w_acc_scores.append(w_acc_val)\n",
    "        print(i)\n",
    "        print(j)\n",
    "        print(w_acc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RTA/**\n",
    "Para el vino blanco utilizamos la distribución rbf. Revisando los resultados de cada combinacion posible, modificando el C y el gamma el mejor ajuste se da cuando c=1000 y gamma=0.01, arrojando un ajuste del 64,12%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6.5\n",
    "\n",
    "Compare the results with other methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.50110602  1.5049741   0.7218695  -0.20022028  0.61319872  1.60677548\n",
      "  -3.26355301  0.88763353  1.23670196 -0.45652364 -1.40947889  0.        ]\n",
      " [ 0.62508096  0.62245144  0.09024381  0.46828107  0.16256151 -0.32138717\n",
      "  -0.26571482 -0.78518054  0.68834373 -0.10778296 -0.68303345  0.        ]\n",
      " [-0.46245766  0.29285521  0.12813349 -0.19594133  0.12911284 -0.18438235\n",
      "   0.58956891  0.36739883 -0.14721951 -0.45584256 -0.76676644  0.        ]\n",
      " [-0.09874784 -0.33423978 -0.26793647 -0.12015716 -0.0210899   0.25553592\n",
      "  -0.4179403   0.28139118 -0.06506546  0.12143296  0.36897705  0.        ]\n",
      " [ 0.55136886 -0.5284807   0.04120626  0.33509974 -0.3234244   0.09979199\n",
      "  -0.48888579 -0.47714007  0.12673663  0.57517202  0.68791151  0.        ]\n",
      " [-0.82985192  0.32133333  0.6351676  -0.12098712 -1.42773986  0.13651082\n",
      "  -0.83947034  0.01842208 -1.03290439  0.63691262  1.28180236  0.        ]]\n",
      "Accuracy: \n",
      " 0.6047529706066291\n",
      "Confusion Matrix: \n",
      " [[  1   1   7   1   0   0]\n",
      " [  0   1  34  16   2   0]\n",
      " [  0   0 533 146   2   0]\n",
      " [  0   0 226 402  10   0]\n",
      " [  0   0  14 155  30   0]\n",
      " [  0   0   0  12   6   0]]\n",
      "f1_Score: \n",
      " 0.6047529706066291\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regresion vino rojo\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg5_a = LogisticRegression(C=1e9, solver='liblinear', multi_class='ovr')\n",
    "logreg5_a.fit(X1_n, Y1)\n",
    "print(logreg5_a.coef_)\n",
    "Y_5_a_predictions = logreg5_a.predict(X1_n)\n",
    "print('Accuracy: \\n', accuracy_score(Y1, Y_5_a_predictions))\n",
    "print('Confusion Matrix: \\n', confusion_matrix(Y1, Y_5_a_predictions))\n",
    "print('f1_Score: \\n', f1_score(Y1, Y_5_a_predictions, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RTA/**Corro una regresión logistica para cada uno de los vino, pára el vino Rojo en Accuracy no mejora, seguimos obteniendo un mejor resultado en Poly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.50110602  1.5049741   0.7218695  -0.20022028  0.61319872  1.60677548\n",
      "  -3.26355301  0.88763353  1.23670196 -0.45652364 -1.40947889  0.        ]\n",
      " [ 0.62508096  0.62245144  0.09024381  0.46828107  0.16256151 -0.32138717\n",
      "  -0.26571482 -0.78518054  0.68834373 -0.10778296 -0.68303345  0.        ]\n",
      " [-0.46245766  0.29285521  0.12813349 -0.19594133  0.12911284 -0.18438235\n",
      "   0.58956891  0.36739883 -0.14721951 -0.45584256 -0.76676644  0.        ]\n",
      " [-0.09874784 -0.33423978 -0.26793647 -0.12015716 -0.0210899   0.25553592\n",
      "  -0.4179403   0.28139118 -0.06506546  0.12143296  0.36897705  0.        ]\n",
      " [ 0.55136886 -0.5284807   0.04120626  0.33509974 -0.3234244   0.09979199\n",
      "  -0.48888579 -0.47714007  0.12673663  0.57517202  0.68791151  0.        ]\n",
      " [-0.82985192  0.32133333  0.6351676  -0.12098712 -1.42773986  0.13651082\n",
      "  -0.83947034  0.01842208 -1.03290439  0.63691262  1.28180236  0.        ]]\n",
      "Accuracy: \n",
      " 0.5377705185790118\n",
      "Confusion Matrix: \n",
      " [[   1    0    8   11    0    0    0]\n",
      " [   0    2   95   64    2    0    0]\n",
      " [   0    0  771  682    4    0    0]\n",
      " [   0    1  399 1741   57    0    0]\n",
      " [   0    0   27  734  119    0    0]\n",
      " [   0    0    2  145   28    0    0]\n",
      " [   0    0    0    3    2    0    0]]\n",
      "f1_Score: \n",
      " 0.5377705185790118\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regresion vino blanco\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg5_b = LogisticRegression(C=1e9, solver='liblinear', multi_class='ovr')\n",
    "logreg5_b.fit(X2_n, Y2)\n",
    "print(logreg5_a.coef_)\n",
    "Y_5_b_predictions = logreg5_b.predict(X2_n)\n",
    "print('Accuracy: \\n', accuracy_score(Y2, Y_5_b_predictions))\n",
    "print('Confusion Matrix: \\n', confusion_matrix(Y2, Y_5_b_predictions))\n",
    "print('f1_Score: \\n', f1_score(Y2, Y_5_b_predictions, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RTA/**Para el vino Blanco en Accuracy no mejora, seguimos obteniendo un mejor resultado en rbf.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6.6\n",
    "\n",
    "\n",
    "* Train a linear regression to predict wine quality (Continous)\n",
    "\n",
    "* Analyze the coefficients\n",
    "\n",
    "* Evaluate the RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"type\"] = data[\"type\"].map({\"white\": 0, \"red\": 1})\n",
    "\n",
    "x6_1 = data['fixed acidity'].values.reshape(-1, 1)\n",
    "x6_2 = data['volatile acidity'].values.reshape(-1, 1)\n",
    "x6_3 = data['citric acid'].values.reshape(-1, 1)\n",
    "x6_4 = data['residual sugar'].values.reshape(-1, 1)\n",
    "x6_5 = data['chlorides'].values.reshape(-1, 1)\n",
    "x6_6 = data['free sulfur dioxide'].values.reshape(-1, 1)\n",
    "x6_7 = data['total sulfur dioxide'].values.reshape(-1, 1)\n",
    "x6_8 = data['density'].values.reshape(-1, 1)\n",
    "x6_9 = data['pH'].values.reshape(-1, 1)\n",
    "x6_10 = data['sulphates'].values.reshape(-1, 1)\n",
    "x6_11 = data['alcohol'].values.reshape(-1, 1)\n",
    "x6_12 = data['type'].values.reshape(-1, 1)\n",
    "X6 = np.c_[x6_1, x6_2, x6_3, x6_4, x6_5, x6_6, x6_7, x6_8, x6_9, x6_10, x6_11, x6_12]\n",
    "\n",
    "ss6 = StandardScaler(with_mean=True, with_std=True)\n",
    "ss6.fit(X6.astype(np.float))\n",
    "X6_n = ss6.transform(X6.astype(np.float))\n",
    "\n",
    "Y6 = data[\"quality\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linreg\n",
      "\n",
      "coef_: \n",
      " [ 0.11027401 -0.24568548 -0.00909927  0.29704168 -0.02652718  0.08762284\n",
      " -0.07927578 -0.311567    0.08018737  0.10739154  0.26556038  0.155642  ]\n",
      "intercept_: \n",
      " 5.81837771279107\n",
      "RMSE: 0.7324\n",
      "r2 score: 0.2965\n"
     ]
    }
   ],
   "source": [
    "# build a linear regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linreg6 = LinearRegression()\n",
    "linreg6.fit(X6_n, Y6)\n",
    "\n",
    "Y_6_predictions = linreg6.predict(X6_n)\n",
    "\n",
    "print('\\nLinreg\\n')\n",
    "\n",
    "print('coef_: \\n', linreg6.coef_)\n",
    "print('intercept_: \\n', linreg6.intercept_)\n",
    "\n",
    "print(\"RMSE: %.4f\" % np.sqrt(mean_squared_error(Y6, Y_6_predictions)))\n",
    "print('r2 score: %.4f' % r2_score(Y6, Y_6_predictions)) # 1 - fit, 0 - no fit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RTA/**Erroe medio cuadratico, entre mas cercano a 0 es porque mas se ajusta el modelo. \n",
    "    r2 score: entre mas cercano a 1, mas se ajusta el modelo \n",
    "Con lo anterio, el modelo no se ajusta con una diferencia muy grande. \n",
    "    El que beta  mas grande es el q mas aporta al momento de definir la calidad de vino (Positivamnete) en este caso es residual sugar y negativamente que es la densidad. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6.7\n",
    "\n",
    "* Estimate a ridge regression with alpha equals 0.1 and 1.\n",
    "* Compare the coefficients with the linear regression\n",
    "* Evaluate the RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ridgereg - 0.1\n",
      "\n",
      "coef_: \n",
      " [ 0.11021354 -0.24567985 -0.00909761  0.29693471 -0.02653296  0.08762743\n",
      " -0.07928723 -0.31140594  0.08015225  0.10738163  0.26562142  0.15558096]\n",
      "intercept_: \n",
      " 5.81837771279107\n",
      "RMSE: 0.7324\n",
      "r2 score: 0.2965\n",
      "\n",
      "Ridgereg - 1\n",
      "\n",
      "coef_: \n",
      " [ 0.10967271 -0.24562903 -0.00908255  0.29597774 -0.02658472  0.08766835\n",
      " -0.07938954 -0.30996565  0.07983816  0.10729294  0.26616683  0.15503466]\n",
      "intercept_: \n",
      " 5.818377712791067\n",
      "RMSE: 0.7324\n",
      "r2 score: 0.2965\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge # alpha=0 is equivalent to linear regression\n",
    "\n",
    "# alpha = 0.1\n",
    "print('\\nRidgereg - 0.1\\n')\n",
    "ridgereg1 = Ridge(alpha=0.1)  # , normalize=True\n",
    "ridgereg1.fit(X6_n, Y6)\n",
    "\n",
    "Y_7_1_predictions = ridgereg1.predict(X6_n)\n",
    "\n",
    "print('coef_: \\n', ridgereg1.coef_)\n",
    "print('intercept_: \\n', ridgereg1.intercept_)\n",
    "\n",
    "print(\"RMSE: %.4f\" % np.sqrt(mean_squared_error(Y6, Y_7_1_predictions)))\n",
    "print('r2 score: %.4f' % r2_score(Y6, Y_7_1_predictions))\n",
    "\n",
    "# alpha = 1\n",
    "print('\\nRidgereg - 1\\n')\n",
    "ridgereg2 = Ridge(alpha=1)  # , normalize=True\n",
    "ridgereg2.fit(X6_n, Y6)\n",
    "\n",
    "Y_7_2_predictions = ridgereg2.predict(X6_n)\n",
    "\n",
    "print('coef_: \\n', ridgereg2.coef_)\n",
    "print('intercept_: \\n', ridgereg2.intercept_)\n",
    "\n",
    "print(\"RMSE: %.4f\" % np.sqrt(mean_squared_error(Y6, Y_7_2_predictions)))\n",
    "print('r2 score: %.4f' % r2_score(Y6, Y_7_2_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6.8\n",
    "\n",
    "* Estimate a lasso regression with alpha equals 0.01, 0.1 and 1.\n",
    "* Compare the coefficients with the linear regression\n",
    "* Evaluate the RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lassoreg - 0.01\n",
      "\n",
      "coef_: \n",
      " [-0.         -0.2266899  -0.          0.08427125 -0.02128916  0.07237462\n",
      " -0.07290899 -0.          0.01170913  0.07755272  0.38765188  0.02836949]\n",
      "intercept_: \n",
      " 5.818377712790538\n",
      "RMSE: 0.7366\n",
      "r2 score: 0.2883\n",
      "\n",
      "Lassoreg - 0.1\n",
      "\n",
      "coef_: \n",
      " [-0.         -0.12133808  0.          0.         -0.          0.\n",
      " -0.         -0.          0.          0.          0.28340642 -0.        ]\n",
      "intercept_: \n",
      " 5.818377712790532\n",
      "RMSE: 0.7641\n",
      "r2 score: 0.2342\n",
      "\n",
      "Lassoreg - 1\n",
      "\n",
      "coef_: \n",
      " [-0. -0.  0. -0. -0.  0. -0. -0.  0.  0.  0. -0.]\n",
      "intercept_: \n",
      " 5.818377712790519\n",
      "RMSE: 0.8732\n",
      "r2 score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso # alpha=0 is equivalent to linear regression\n",
    "\n",
    "# alpha = 0.01\n",
    "print('\\nLassoreg - 0.01\\n')\n",
    "lassoreg1 = Lasso(alpha=0.01)  # , normalize=True\n",
    "lassoreg1.fit(X6_n, Y6)\n",
    "\n",
    "Y_8_1_predictions = lassoreg1.predict(X6_n)\n",
    "\n",
    "print('coef_: \\n', lassoreg1.coef_)\n",
    "print('intercept_: \\n', lassoreg1.intercept_)\n",
    "\n",
    "print(\"RMSE: %.4f\" % np.sqrt(mean_squared_error(Y6, Y_8_1_predictions)))\n",
    "print('r2 score: %.4f' % r2_score(Y6, Y_8_1_predictions))\n",
    "\n",
    "# alpha = 0.1\n",
    "print('\\nLassoreg - 0.1\\n')\n",
    "lassoreg2 = Lasso(alpha=0.1)  # , normalize=True\n",
    "lassoreg2.fit(X6_n, Y6)\n",
    "\n",
    "Y_8_2_predictions = lassoreg2.predict(X6_n)\n",
    "\n",
    "print('coef_: \\n', lassoreg2.coef_)\n",
    "print('intercept_: \\n', lassoreg2.intercept_)\n",
    "\n",
    "print(\"RMSE: %.4f\" % np.sqrt(mean_squared_error(Y6, Y_8_2_predictions)))\n",
    "print('r2 score: %.4f' % r2_score(Y6, Y_8_2_predictions))\n",
    "\n",
    "# alpha = 1\n",
    "print('\\nLassoreg - 1\\n')\n",
    "lassoreg3 = Lasso(alpha=1)  # , normalize=True\n",
    "lassoreg3.fit(X6_n, Y6)\n",
    "\n",
    "Y_8_3_predictions = lassoreg3.predict(X6_n)\n",
    "\n",
    "print('coef_: \\n', lassoreg3.coef_)\n",
    "print('intercept_: \\n', lassoreg3.intercept_)\n",
    "\n",
    "print(\"RMSE: %.4f\" % np.sqrt(mean_squared_error(Y6, Y_8_3_predictions)))\n",
    "print('r2 score: %.4f' % r2_score(Y6, Y_8_3_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RTA/** En este modelo de clasificacion los betas los va eliminado si no son gratificantes para el modelo. Como se observa cuando el alpha es 1 los betas se eliminan, el RMSE con mayor ajuste al modelo es alpha 0.01. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6.9\n",
    "\n",
    "* Create a binary target\n",
    "\n",
    "* Train a logistic regression to predict wine quality (binary)\n",
    "\n",
    "* Analyze the coefficients\n",
    "\n",
    "* Evaluate the f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.64040034 -0.60303914 -0.03615616  1.04543735 -0.26664871  0.19165801\n",
      "  -0.20940333 -1.27058901  0.4174233   0.36582077  0.54161572  0.33368393]]\n",
      "Histograma: \n",
      " 0    5220\n",
      "1    1277\n",
      "Name: quality, dtype: int64\n",
      "Accuracy: \n",
      " 0.8182237955979683\n",
      "Confusion Matrix: \n",
      " [[4974  246]\n",
      " [ 935  342]]\n",
      "f1_Score: \n",
      " 0.36675603217158176\n"
     ]
    }
   ],
   "source": [
    "Y7 = data[\"quality\"].map({3: 0, 4: 0, 5: 0, 6: 0, 7: 1, 8: 1, 9: 1})\n",
    "\n",
    "logreg9 = LogisticRegression(C=1e9,  solver='liblinear')\n",
    "logreg9.fit(X6_n, Y7)\n",
    "print(logreg9.coef_)\n",
    "\n",
    "Y_9_predictions = logreg9.predict(X6_n)\n",
    "print('Histograma: \\n', pd.value_counts(Y7).sort_index())\n",
    "print('Accuracy: \\n', accuracy_score(Y7, Y_9_predictions))\n",
    "print('Confusion Matrix: \\n', confusion_matrix(Y7, Y_9_predictions))\n",
    "print('f1_Score: \\n', f1_score(Y7, Y_9_predictions)) #  , average='micro' - Calculate metrics globally by counting the total true positives, false negatives and false positives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RTA**El modelo tiene buena prediccion, el f1score por mattiz confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6.10\n",
    "\n",
    "* Estimate a regularized logistic regression using:\n",
    "* C = 0.01, 0.1 & 1.0\n",
    "* penalty = ['l1, 'l2']\n",
    "* Compare the coefficients and the f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 0.01\n",
      "L1 lasso\n",
      "[[ 0.         -0.35079754  0.          0.         -0.0242809   0.\n",
      "   0.          0.          0.          0.07960373  0.84114512  0.        ]]\n",
      "Accuracy: \n",
      " 0.8157611205171618\n",
      "f1_Score: \n",
      " 0.28108108108108104\n",
      "L2, ri\n",
      "[[ 0.18429336 -0.37887596  0.02103926  0.31978012 -0.20735008  0.15011426\n",
      "  -0.18026332 -0.31735527  0.14784796  0.23374954  0.70558531  0.01590001]]\n",
      "Accuracy: \n",
      " 0.8197629675234723\n",
      "f1_Score: \n",
      " 0.3417650365373805\n",
      "C = 0.1\n",
      "L1\n",
      "[[ 0.35521794 -0.55150504 -0.00386492  0.59463244 -0.23841997  0.17567032\n",
      "  -0.21910094 -0.57972344  0.25703126  0.30527712  0.79849787  0.07731327]]\n",
      "Accuracy: \n",
      " 0.8199168847160228\n",
      "f1_Score: \n",
      " 0.3578485181119649\n",
      "L2\n",
      "[[ 0.45774901 -0.5690227  -0.02778742  0.74526984 -0.27643395  0.19688836\n",
      "  -0.22960076 -0.8108624   0.31236366  0.32510428  0.69935354  0.18108278]]\n",
      "Accuracy: \n",
      " 0.817762044020317\n",
      "f1_Score: \n",
      " 0.35791757049891537\n",
      "C = 1\n",
      "L1\n",
      "[[ 0.61003971 -0.59747018 -0.03274185  0.99727367 -0.26367716  0.19005676\n",
      "  -0.21068431 -1.19648316  0.40034149  0.35931877  0.56901193  0.30629196]]\n",
      "Accuracy: \n",
      " 0.8179159612128675\n",
      "f1_Score: \n",
      " 0.3643202579258463\n",
      "L2\n",
      "[[ 0.61157843 -0.59971206 -0.03546809  0.99750967 -0.26960691  0.19321553\n",
      "  -0.21375256 -1.19484042  0.40092168  0.35970188  0.56968253  0.30871198]]\n",
      "Accuracy: \n",
      " 0.8173002924426659\n",
      "f1_Score: \n",
      " 0.36285560923242083\n"
     ]
    }
   ],
   "source": [
    "print (\"C = 0.01\")\n",
    "print (\"L1\", \"lasso\")\n",
    "\n",
    "logreg10_1_a = LogisticRegression(C=0.01, penalty='l1', solver='liblinear')\n",
    "logreg10_1_a.fit(X6_n, Y7)\n",
    "print(logreg10_1_a.coef_)\n",
    "\n",
    "Y_10_1_a_predictions = logreg10_1_a.predict(X6_n)\n",
    "print('Accuracy: \\n', accuracy_score(Y7, Y_10_1_a_predictions))\n",
    "print('f1_Score: \\n', f1_score(Y7, Y_10_1_a_predictions)) # Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "\n",
    "print(\"L2, ri\")\n",
    "logreg10_1_b = LogisticRegression(C=0.01, penalty='l2', solver='liblinear')\n",
    "logreg10_1_b.fit(X6_n, Y7)\n",
    "print(logreg10_1_b.coef_)\n",
    "\n",
    "Y_10_1_b_predictions = logreg10_1_b.predict(X6_n)\n",
    "print('Accuracy: \\n', accuracy_score(Y7, Y_10_1_b_predictions))\n",
    "print('f1_Score: \\n', f1_score(Y7, Y_10_1_b_predictions)) # Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "\n",
    "print(\"C = 0.1\")\n",
    "print(\"L1\")\n",
    "logreg10_2_a = LogisticRegression(C=0.1, penalty='l1', solver='liblinear')\n",
    "logreg10_2_a.fit(X6_n, Y7)\n",
    "print(logreg10_2_a.coef_)\n",
    "\n",
    "Y_10_2_a_predictions = logreg10_2_a.predict(X6_n)\n",
    "print('Accuracy: \\n', accuracy_score(Y7, Y_10_2_a_predictions))\n",
    "print('f1_Score: \\n', f1_score(Y7, Y_10_2_a_predictions)) # Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "\n",
    "print(\"L2\")\n",
    "logreg10_2_b = LogisticRegression(C=0.1, penalty='l2', solver='liblinear')\n",
    "logreg10_2_b.fit(X6_n, Y7)\n",
    "print(logreg10_2_b.coef_)\n",
    "\n",
    "Y_10_2_b_predictions = logreg10_2_b.predict(X6_n)\n",
    "print('Accuracy: \\n', accuracy_score(Y7, Y_10_2_b_predictions))\n",
    "print('f1_Score: \\n', f1_score(Y7, Y_10_2_b_predictions)) # Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "\n",
    "print (\"C = 1\")\n",
    "print(\"L1\")\n",
    "logreg10_3_a = LogisticRegression(C=1, penalty='l1', solver='liblinear')\n",
    "logreg10_3_a.fit(X6_n, Y7)\n",
    "print(logreg10_3_a.coef_)\n",
    "\n",
    "Y_10_3_a_predictions = logreg10_3_a.predict(X6_n)\n",
    "print('Accuracy: \\n', accuracy_score(Y7, Y_10_3_a_predictions))\n",
    "print('f1_Score: \\n', f1_score(Y7, Y_10_3_a_predictions)) # Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "\n",
    "print(\"L2\")\n",
    "logreg10_3_b = LogisticRegression(C=1, penalty='l2', solver='liblinear')\n",
    "logreg10_3_b.fit(X6_n, Y7)\n",
    "print(logreg10_3_b.coef_)\n",
    "\n",
    "Y_10_3_b_predictions = logreg10_3_b.predict(X6_n)\n",
    "print('Accuracy: \\n', accuracy_score(Y7, Y_10_3_b_predictions))\n",
    "print('f1_Score: \\n', f1_score(Y7, Y_10_3_b_predictions)) # Calculate metrics globally by counting the total true positives, false negatives and false positives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RTA/** Con un C=0.01 los coeficientes se hacen 0. De igual manera de acuerdo al r1_score el mejor es con C=1 y para penalty = L1. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
